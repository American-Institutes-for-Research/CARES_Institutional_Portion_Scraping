{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate the Grantee Website Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Packages\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "import progressbar\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make a Python list of URLs to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4715"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a List of websites to monitor by typing them in to a CSV file stored in the same directory as this program.\n",
    "#This list is named csv_urls.\n",
    "\n",
    "filename = open('test.csv', 'r')\n",
    "file = csv.DictReader(filename)\n",
    "csv_urls = {}\n",
    "\n",
    "for row in file:\n",
    "    url = row['url']\n",
    "    if \"http\" not in url:\n",
    "        csv_urls[row['name']] = \"https://\" + url\n",
    "    elif \"https\" not in url:\n",
    "        csv_urls[row['name']] = \"https://\" + url[7:]\n",
    "    else:\n",
    "        csv_urls[row['name']] = url\n",
    "\n",
    "# len(csv_urls)\n",
    "# print(csv_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run through the URLs to see if they are active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "#function get_url_status uses the requests package (installed above) to find status of site\n",
    "#HTTP response status codes of 400 and above may be problems for the Program Office.\n",
    "#Informational responses (100–199) Successful responses (200–299) Redirects (300–399)\n",
    "#Client errors (400–499) Server errors (500–599)\n",
    "\n",
    "#ensure not on VPN when running this\n",
    "\n",
    "def get_url_status(urls):  # checks status for each url in dict of urls    \n",
    "    url_statuses = {} #maps {name of school: (url, status code, request.get)}\n",
    "    #create a progress bar to monitor progress through urls\n",
    "    bar = progressbar.ProgressBar(maxval=len(urls), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    done = 0\n",
    "    for school_name in urls.keys():\n",
    "        for i in range(3):\n",
    "            while True:\n",
    "                try:\n",
    "                    r = requests.get(urls[school_name], timeout=15)\n",
    "                    url_statuses[school_name] = (urls[school_name], str(r.status_code), r)\n",
    "                except Exception as e:\n",
    "                    url_statuses[school_name] = (urls[school_name], e,)\n",
    "                break\n",
    "        done += 1\n",
    "        bar.update(done)\n",
    "    bar.finish()\n",
    "    return url_statuses\n",
    "\n",
    "# url_statuses = get_url_status(csv_urls) #uncomment to generate trial csv\n",
    "# print(get_url_status(csv_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM INACTIVE:  1134\n"
     ]
    }
   ],
   "source": [
    "#takes in a list of urls and uses the get_url_status function to return a dict of active urls\n",
    "#dict of the form {name of school: (url, status code, request.get)}\n",
    "\n",
    "def get_active_urls(urls):\n",
    "    url_statuses = get_url_status(urls) #comment to generate trial csv\n",
    "    active_urls = {}\n",
    "    inactive_urls = {}\n",
    "    for school_name in urls.keys():\n",
    "        if url_statuses[school_name][1] == \"200\":\n",
    "            active_urls[school_name] = url_statuses[school_name]\n",
    "        else:\n",
    "            inactive_urls[school_name] = url_statuses[school_name][1]\n",
    "            # print(school_name, urls[school_name])\n",
    "    if len(inactive_urls) > 0:\n",
    "#         print(\"INACTIVE URLS: \", inactive_urls)\n",
    "        print(\"NUM INACTIVE: \", len(inactive_urls))\n",
    "    return active_urls\n",
    "\n",
    "# get_active_urls(csv_urls)\n",
    "# active_urls = get_active_urls(csv_urls) #uncomment to generate trial csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n403 errors: https://stackoverflow.com/questions/38489386/python-requests-403-forbidden (couldn't figure this out)\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "403 errors: https://stackoverflow.com/questions/38489386/python-requests-403-forbidden (couldn't figure this out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download quarterly report pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting active urls\n",
      "dowloading pdfs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e0cc7806fc56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NUMBER PDFS:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_pdfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindPDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_urls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactive_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-e0cc7806fc56>\u001b[0m in \u001b[0;36mfindPDF\u001b[1;34m(urls, active_urls)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactive_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mschool_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Parse text obtained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;31m# Fallback to auto-detected encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;31m# Decode unicode from given encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mapparent_encoding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapparent_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[1;34m\"\"\"The apparent encoding, provided by the chardet library.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_unicode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[0;32m    213\u001b[0m                                    \u001b[1;34m'confidence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\sbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    116\u001b[0m                         \u001b[0mlm_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_order\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_seq_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlm_cat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_order\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mcharset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharset_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def findPDF(urls, active_urls):  # finds PDF in site and saves it\n",
    "    print('getting active urls')\n",
    "    active_urls = get_active_urls(urls) #comment to generate trial csv\n",
    "    pdfs = {}\n",
    "    print('dowloading pdfs')\n",
    "    no_pdfs = []\n",
    "    num_pdfs = {}\n",
    "    \n",
    "    bar = progressbar.ProgressBar(maxval=len(urls), widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    done = 0\n",
    "    \n",
    "    for school_name in active_urls.keys():\n",
    "        # print(school_name)\n",
    "        # print(active_urls[school_name][0])\n",
    "        number = 0\n",
    "        r = active_urls[school_name][2]\n",
    "        # Parse text obtained\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            # pick up only those URLs containing 'pdf'\n",
    "            # within 'href' attribute\n",
    "            if \"quarterly\" in str(link) or \"Quarterly\" in str(link):\n",
    "                number += 1\n",
    "                # init PDF url\n",
    "                pdf_url = ''\n",
    "                # append base URL if no 'https' available in URL\n",
    "                if 'https' not in link.get('href'):\n",
    "                    pdf_url = urljoin(active_urls[school_name][0], link.get('href'))\n",
    "\n",
    "                # otherwise use bare URL\n",
    "                else:\n",
    "                    pdf_url = link.get('href')\n",
    "                \n",
    "                pdf_url = pdf_url.replace(\" \", \"%20\")\n",
    "\n",
    "                # print('HTTP GET: ', pdf_url)\n",
    "\n",
    "                # extract PDF file name\n",
    "                filename = \"testpdfs/\" + school_name.replace(' ', '_') + \"__\" + pdf_url.split('/')[-1].replace('%20','_')\n",
    "                # print(filename)\n",
    "                \n",
    "                # write PDF to local file\n",
    "                pdf_errors = {}\n",
    "                try:\n",
    "                    pdf = urlopen(pdf_url)\n",
    "                    file = open(filename, 'wb')\n",
    "                    file.write(pdf.read())\n",
    "                    file.close()\n",
    "                except:\n",
    "                    if link in pdf_errors:\n",
    "                        pdf_errors[link].append(filename)\n",
    "                    else:\n",
    "                        pdf_errors[link] = [filename]\n",
    "        if number == 0:\n",
    "            no_pdfs.append(school_name)\n",
    "        num_pdfs[school_name] = number\n",
    "        done += 1\n",
    "        bar.update(done)\n",
    "    bar.finish()\n",
    "#     return num_pdfs #uncomment to generate trial csv\n",
    "    print(\"PDF ERRORS: \", pdf_errors)\n",
    "    print(\"NO PDFS FOUND: \", no_pdfs)\n",
    "    print(\"NUMBER PDFS:\", num_pdfs)\n",
    "    \n",
    "# number = findPDF(csv_urls, active_urls) #uncomment to generate trial csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to distinguish between institutional and student reports somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #generate csv with results from trial\n",
    "# trial = []\n",
    "# # trial = [{'school_name': i, 'status': url_statuses[i][1]} for i in url_statuses.keys()]\n",
    "# for school_name in csv_urls.keys():\n",
    "#     school_dict = {'school_name': school_name}\n",
    "#     if school_name in active_urls.keys():\n",
    "#         school_dict['status'] = url_statuses[school_name][1]\n",
    "#         school_dict['active_url'] = 1\n",
    "# #         school_dict['num_pdfs'] = number[school_name]\n",
    "#     else:\n",
    "#         school_dict['status'] = url_statuses[school_name][1]\n",
    "#         school_dict['active_url'] = 0\n",
    "#         school_dict['num_pdfs'] = 0    \n",
    "#     trial.append(school_dict)\n",
    "\n",
    "# fieldnames = ['school_name', 'status', 'active_url', 'num_pdfs']\n",
    "\n",
    "# with open(\"trial.csv\", 'w', encoding='UTF8', newline='') as f:\n",
    "#     writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "# #     writer = csv.DictWriter(f, fieldnames=['school_name', 'status'])\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting active urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INACTIVE URLS:  {'ILISAGVIK COLLEGE': '403', 'ALABAMA STATE UNIVERSITY': '404', 'ATHENS STATE UNIVERSITY': '403', 'AUBURN UNIVERSITY AT MONTGOMERY': '403', 'BIRMINGHAM-SOUTHERN COLLEGE': '406', 'BROWN BEAUTY BARBER SCHOOL': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='www.covid-relief-data.ed', port=443): Max retries exceeded with url: /gov (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001B804ED7B80>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\")), 'DONALD S MATHEWS DBA AL ST COLLEGE OF BARBER STYLING': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'alabamastatebarbercollege.com\\', port=443): Max retries exceeded with url: /heerf.html (Caused by SSLError(SSLCertVerificationError(\"hostname \\'alabamastatebarbercollege.com\\' doesn\\'t match either of \\'*.accountservergroup.com\\', \\'accountservergroup.com\\'\")))')), 'GEORGE CORLEY WALLACE STATE COMMUNITY COLLEGE': '404', 'H. COUNCILL TRENHOLM STATE COMMUNITY COLLEGE': InvalidURL(\"Invalid URL 'https://': No host supplied\"), 'HUNTSVILLE BIBLE COLLEGE': '406', 'JACKSONVILLE STATE UNIVERSITY': SSLError(MaxRetryError(\"HTTPSConnectionPool(host='www.jsu.edu', port=443): Max retries exceeded with url: /coronavirus/cockycares/reporting/index.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1125)')))\")), 'JEFFERSON STATE COMMUNITY COLLEGE': '403'}\n",
      "NUM INACTIVE:  12\n",
      "dowloading pdfs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF ERRORS:  {<a href=\"https://www.judson.edu/wp-content/uploads/2021/04/HEERF-3-31-21-quarterly-report-4-7-2021.pdf\">Quarterly Report – 3.31.2021</a>: ['testpdfs/JUDSON_COLLEGE__HEERF-3-31-21-quarterly-report-4-7-2021.pdf']}\n",
      "NO PDFS FOUND:  ['ALASKA CAREER COLLEGE', 'ALASKA PACIFIC UNIVERSITY', 'ALABAMA A&M UNIVERSITY', 'ALABAMA SCHOOL OF NAIL TECHNOLOGY AND COSMETOLOGY INC', 'AUBURN UNIVERSITY', 'BEVILL STATE COMMUNITY COLLEGE', 'BISHOP STATE COMMUNITY COLLEGE', 'CARDIAC AND VASCULAR INSTITUTE OF ULTRASOUND, INC.', 'CHATTAHOOCHEE VALLEY COMMUNITY COLLEGE', 'GADSDEN STATE COMMUNITY COLLEGE', 'HERITAGE CHRISTIAN UNIVERSITY', 'J. F. DRAKE STATE COMMUNITY AND TECHNICAL COLLEGE', 'J.F. INGRAM STATE TECHNICAL COLLEGE', 'LAWSON STATE COMMUNITY COLLEGE', 'LURLEEN B. WALLACE COMMUNITY COLLEGE', 'MARION MILITARY INSTITUTE', 'MIDFIELD INSTITUTE OF COSMETOLOGY INC', 'MILES COLLEGE', 'NORTHEAST ALABAMA COMMUNITY COLLEGE', 'NORTHWEST-SHOALS COMMUNITY COLLEGE']\n",
      "NUMBER PDFS: {'ALASKA BIBLE COLLEGE': 1, 'ALASKA CAREER COLLEGE': 0, 'ALASKA CHRISTIAN COLLEGE': 3, 'ALASKA PACIFIC UNIVERSITY': 0, 'ALASKA VOCATIONAL TECHNICAL CENTER': 3, 'UNIVERSITY OF ALASKA ANCHORAGE': 3, 'UNIVERSITY OF ALASKA FAIRBANKS': 3, 'UNIVERSITY OF ALASKA SOUTHEAST': 5, 'ALABAMA A&M UNIVERSITY': 0, 'ALABAMA SCHOOL OF NAIL TECHNOLOGY AND COSMETOLOGY INC': 0, 'AUBURN UNIVERSITY': 0, 'BEVILL STATE COMMUNITY COLLEGE': 0, 'BISHOP STATE COMMUNITY COLLEGE': 0, 'BLUE CLIFF CAREER COLLEGE': 1, 'CARDIAC AND VASCULAR INSTITUTE OF ULTRASOUND, INC.': 0, 'CAREER CONSULTANTS INC.': 86, 'CENTRAL ALABAMA COMMUNITY COLLEGE': 1, 'CHATTAHOOCHEE VALLEY COMMUNITY COLLEGE': 0, 'ENTERPRISE STATE COMMUNITY COLLEGE': 3, 'FAULKNER UNIVERSITY': 3, 'FORTIS COLLEFE/ EDUATION AFFILIAES INC': 86, 'FORTIS COLLEGE MOBILE/ EA INC': 86, 'GADSDEN STATE COMMUNITY COLLEGE': 0, 'GEORGE C. WALLACE COMMUNITY COLLEGE (DOTHAN, AL)': 3, 'HERITAGE CHRISTIAN UNIVERSITY': 0, 'HUNTINGDON COLLEGE': 3, 'J. F. DRAKE STATE COMMUNITY AND TECHNICAL COLLEGE': 0, 'J.F. INGRAM STATE TECHNICAL COLLEGE': 0, 'JC CALHOUN STATE COMMUNITY COLLEGE': 7, 'JUDSON COLLEGE': 5, 'LAWSON STATE COMMUNITY COLLEGE': 0, 'LURLEEN B. WALLACE COMMUNITY COLLEGE': 0, 'MARION MILITARY INSTITUTE': 0, 'MIDFIELD INSTITUTE OF COSMETOLOGY INC': 0, 'MILES COLLEGE': 0, 'NORTHEAST ALABAMA COMMUNITY COLLEGE': 0, 'NORTHWEST-SHOALS COMMUNITY COLLEGE': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "findPDF(csv_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
